{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#imports\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecção de Objetos \n",
    "\n",
    "Em visáo computacional, existem três tipos de aplicações principais: classificação, detecção e segmentação. Em classificação, dada uma imagem, a tarefa principal é reconhecer o objeto que está na imagem, como por exemplo, um gato. Por outro lado, caso o objetivo seja não apenas reconhecer o objeto como também localizá-lo na imagem, teremos um problema de detecção. Problemas de detecção, em geral, ao localizar uma imagem, incluem um retângulo (i.e. bounding box) que determina tudo que o algoritmo entende na imagem que represente este objeto. \n",
    "\n",
    "Ao adicionar as bounding boxes, percebe-se que fatalmente algumas porções do retângulo não representarão o objeto, a menos que o objeto também seja quadrado, Dessa forma, existe uma terceira aplicação, denominada segmentação, em que apenas os pixels que representam o objeto são selecionados. \n",
    "\n",
    "![alt text](../imgs/object_detection.jpeg \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haar Cascade Classifier\n",
    "\n",
    "Em visão computacional clássica, existe um classificador que utiliza algoritmo de aprendizagem de máquina (Adaboost) para análisar features da imagem.\n",
    "\n",
    "### E o que são features?\n",
    "\n",
    "Uma imagem é representada por uma matrix, sendo cada pixel um elemento desta matriz. A combinação de pixels semelhantes pode representar uma determinada característica de um objeto, denominada feature. Ao utilizar o Haar Cascade, o algoritmo percorre a matriz aplicando operações matemáticas (e.g. convolução) com a finalidade de identificar essas características e identificar as features. Ao realizar essas operações, o algoritmo identifica formas das features (i.e. linear, bordas horizontais, diagonais e verticais). \n",
    "\n",
    "![alt text](../imgs/lena.gif \"Title\")\n",
    "\n",
    "Para reconhecimento facial, por exemplo, essas features podem ser olhos (formados por um conjunto de features horizontais), nariz (combinação de bordas verticais e diagonais) e boca (features horizontais).\n",
    "\n",
    "![alt text](../imgs/features.png \"Title\")\n",
    "\n",
    "Para identificar as features de forma que obtenha resultados satisfatórios e de forma rápida, o Haar possui diferentes estágios, sendo cada um deles uma parte da imagem com features (janela). Para cada janela, o classificador Adaboost é utilizado e, caso features correspondentes ao objeto sejam encontradas, a janela é utilizada, caso contrário, é rejeitada.\n",
    "\n",
    "![alt text](../imgs/cascade.png \"Title\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagens e OpenCV\n",
    "\n",
    "Como dito anteriormente, cada imagem é representada por uma matriz. Essa matriz poderá ser 2D caso a imagem seja binária, ou 3D caso a imagem seja em escala de cinza ou RGB. Para ambos os casos, as duas primeiras dimensões representam respectivamente linha e coluna da imagem e, para matrizes 3D, a terceira dimensão representará o canal de cor. Para imagens RGB, a terceira dimensão representará cada canal de cor (R = red, G = green e B = blue).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Leitura de imagem em raw\n",
    "img_raw = cv2.imread('../imgs/test/ceci.jpg')\n",
    "print(type(img_raw)) \n",
    "print(img_raw.shape)\n",
    "plt.imshow(img_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Leitura de imagem RGB \n",
    "img = cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB)\n",
    "print(type(img_raw))\n",
    "print(img_raw.shape)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura de imagem em escaladas de Cinza\n",
    "img = cv2.cvtColor(img_raw, cv2.COLOR_BGR2GRAY)\n",
    "print(type(img_raw))\n",
    "print(img_raw.shape)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exibindo imagens nos canais R, G e B\n",
    "img_raw = cv2.imread('../imgs/test/ceci.jpg')\n",
    "#Passo 1 - Crie uma cópia da imagem para cada canal\n",
    "r = img_raw.copy()\n",
    "b = img_raw.copy()\n",
    "g = img_raw.copy()\n",
    "\n",
    "#Passo 2 - Para cada canal, zere na matriz os canais que não serão utilizados\n",
    "# para R, zere os canais 0 e 1.\n",
    "r[:, :, 0] = 0\n",
    "r[:, :, 1] = 0\n",
    "# para G, zere canais 0 e 2 \n",
    "g[:, :, 0] = 0\n",
    "g[:, :, 2] = 0\n",
    "# para B, zere os canais 1 e 2\n",
    "b[:, :, 1] = 0\n",
    "b[:, :, 2] = 0\n",
    "\n",
    "# RGB - Blue\n",
    "plt.imshow(b)\n",
    "plt.show()\n",
    "# RGB - Green\n",
    "plt.imshow(g)\n",
    "plt.show()\n",
    "# RGB - Red\n",
    "plt.imshow(r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando o Haar Cascade\n",
    "\n",
    "Para aplicar o Haar, é necessário primeiramente obter um arquivo xml correspondente ao modelo do classificador obtido com o treinamento. O treino é a etapa em que o algoritmo irá analisar as imagens e compreender o que representa as features dos objetos. Essas imagens são dividas no mínimo em duas classes: i. imagens positivas (i.e. que representam o objeto a ser classificado) ii. imagens negativas (i.e. que representam qualquer outro objeto que não seja o objeto a ser classificado).\n",
    "\n",
    "Existem duas formas de utilizar o classificador: utilizando um modelo pré-treinado ou treinando o próprio modelo. Para o modelo pré-treinado, o arquivo XML já estará gerado e a poderemos classificar novas imagens, enquanto para treinar o próprio modelo, o arquivo XML será gerado após o treinamento. \n",
    "\n",
    "O OpenCV disponibiliza modelos pré-treinados que podem ser utilizados para determinados objetos, entretanto, caso o objeto seja muito específico, um novo modelo deverá ser treinado. Para isso, uma base de dados de treino deverá ser gerada com os exemplos positivos e negativos balanceados. Para gerar a base, será necessário obter: i) as imagens, ii) a classe correspondente a imagem (e.g. 0 para rosto e 1 para não rosto), e iii) a localização do objeto, representada por quatro parâmetros (x,y,width, height). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando modelo pré-treinado do OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Passo 1 - Leitura do XML\n",
    "face_cascade = cv2.CascadeClassifier('../data/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('../data/haarcascade_eye.xml')\n",
    "### Passo 2 - Leitura da imagem\n",
    "img = cv2.imread('../imgs/test/robot.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "### Passo 3 - Criar um objeto com o modelo cascade para a imagem. Isso é feito por meio do método\n",
    "# detectMultiScale e possui três parâmetros: a imagem, o scaleFactor e o num_neighboors.\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "### Passo 4 - Identificar regiões de interesse (ROI) na imagem\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "    for (ex,ey,ew,eh) in eyes:\n",
    "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "cv2.imshow('img',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
